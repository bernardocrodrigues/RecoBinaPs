{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Recommender Systems\n",
    "\n",
    "This notebook exemplifies how to evaluate the performance of a recommender\n",
    "system using the implementation from evaluation.benchmark module.\n",
    "\n",
    "Copyright 2024 Bernardo C. Rodrigues\n",
    "\n",
    "See COPYING file for license details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup notebook\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluation.plot as plot\n",
    "\n",
    "# Load the autoreload extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# Set autoreload to reload all modules every time a cell is executed\n",
    "%autoreload 2\n",
    "\n",
    "# Call the function to customize the default template\n",
    "plot.customize_default_template()\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already downloaded!. Nothing to do.\n"
     ]
    }
   ],
   "source": [
    "from dataset.common import resolve_folds\n",
    "from dataset.movie_lens import load_ml_100k_folds\n",
    "\n",
    "data, k_fold = load_ml_100k_folds()\n",
    "folds = resolve_folds(data, k_fold)\n",
    "\n",
    "folds_without_index = [fold[1] for fold in folds]\n",
    "\n",
    "_, (trainset, testset) = folds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.strategies import (\n",
    "    MAEStrategy,\n",
    "    RMSEStrategy,\n",
    "    MicroAveragedRecallStrategy,\n",
    "    MacroAveragedRecallStrategy,\n",
    "    RecallAtKStrategy,\n",
    "    MicroAveragedPrecisionStrategy,\n",
    "    MacroAveragedPrecisionStrategy,\n",
    "    PrecisionAtKStrategy,\n",
    "    F1ScoreStrategy,\n",
    "    NDCGStrategy,\n",
    "    PredictionCoverageStrategy,\n",
    ")\n",
    "\n",
    "train_measures = [ ]\n",
    "\n",
    "test_measures = [\n",
    "    MAEStrategy(verbose=False),\n",
    "    RMSEStrategy(verbose=False),\n",
    "    MicroAveragedRecallStrategy(threshold=4.0),\n",
    "    MacroAveragedRecallStrategy(threshold=4.0),\n",
    "    RecallAtKStrategy(k=20, threshold=4.0),\n",
    "    MicroAveragedPrecisionStrategy(threshold=4.0),\n",
    "    MacroAveragedPrecisionStrategy(threshold=4.0),\n",
    "    PrecisionAtKStrategy(k=20, threshold=4.0),\n",
    "    F1ScoreStrategy(k=20, threshold=4.0),\n",
    "    NDCGStrategy(k=20, threshold=4.0),\n",
    "    PredictionCoverageStrategy(),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_and_score will benchmark the recommender system against a single fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 20000/20000 | Avg. time/task: 0m 0.0s | Time left: 0m 0.0s\n",
      "All tasks completed.\n",
      "Total time: 0h 0m 3.8s\n",
      "mae                           |  0.750\n",
      "rmse                          |  0.951\n",
      "micro_averaged_recall         |  0.383\n",
      "macro_averaged_recall         |  0.334\n",
      "recall_at_20                  |  0.372\n",
      "micro_averaged_precision      |  0.842\n",
      "macro_averaged_precision      |  0.696\n",
      "precision_at_20               |  0.770\n",
      "f1_score                      |  0.502\n",
      "nDCG_at_20                    |  0.443\n",
      "prediction_coverage           |  1.000\n",
      "Fit time:                     |  0.276\n",
      "Test time:                    |  3.846\n"
     ]
    }
   ],
   "source": [
    "from evaluation.benchmark import fit_and_score\n",
    "from surprise.prediction_algorithms import SVD\n",
    "\n",
    "recommender = SVD()\n",
    "\n",
    "test_measurements, train_measurements, fit_time, test_time = fit_and_score(\n",
    "    recommender_system=recommender,\n",
    "    trainset=trainset,\n",
    "    testset=testset,\n",
    "    test_measures=test_measures,\n",
    "    train_measures=train_measures,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for measure, measurement in test_measurements.items():\n",
    "    print(f\"{measure:<30}|  {measurement:.3f}\")\n",
    "\n",
    "print(f\"Fit time:                     |  {fit_time:.3f}\")\n",
    "print(f\"Test time:                    |  {test_time:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validate will benchmark the recommender system against multiple folds and return a list of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed 5/5 | Avg. time/task: 0m 0.2s | Time left: 0m 0.0s | Estimated completion time: 17:09:28\n",
      "All tasks completed.\n",
      "Total time: 0h 0m 1.1s\r"
     ]
    }
   ],
   "source": [
    "from evaluation.benchmark import cross_validate\n",
    "\n",
    "measurements = cross_validate(\n",
    "    recommender_system=recommender,\n",
    "    folds=folds_without_index,\n",
    "    test_measures=test_measures,\n",
    "    train_measures=train_measures,\n",
    "    max_workers=16,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure, measurement in measurements.items():\n",
    "    print(f\"{measure:<30}|  {measurement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validate_recommenders will benchmark the list of recommenders using the provided folds and measures. The function will return a dictionary with the measurements for each recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms import SVD\n",
    "from evaluation.benchmark import cross_validade_recommenders\n",
    "\n",
    "recommenders = [\n",
    "    SVD(n_factors=10, n_epochs=10),\n",
    "    SVD(n_factors=10, n_epochs=20),\n",
    "    SVD(n_factors=20, n_epochs=20),\n",
    "    SVD(n_factors=20, n_epochs=40),\n",
    "    SVD(n_factors=40, n_epochs=40),\n",
    "]\n",
    "\n",
    "recommenders_measurements = cross_validade_recommenders(\n",
    "    recommenders=recommenders,\n",
    "    folds=folds_without_index,\n",
    "    test_measures=test_measures,\n",
    "    train_measures=train_measures,\n",
    "    max_workers=16,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recommender_measurements in recommenders_measurements:\n",
    "    for measure, measurement in recommender_measurements.items():\n",
    "        print(f\"{measure:<30}|  {measurement}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms import SVD\n",
    "from evaluation.benchmark import GridSearch\n",
    "\n",
    "parameters_grid = {\n",
    "    \"n_factors\": [50, 100, 200],\n",
    "    \"n_epochs\": [10, 20, 40],\n",
    "    \"biased\": [True, False],\n",
    "}\n",
    "\n",
    "grid_search = GridSearch(\n",
    "    SVD,\n",
    "    parameters_grid,\n",
    "    test_measures,\n",
    "    train_measures,\n",
    "    max_workers=16,\n",
    ")\n",
    "\n",
    "best, ordering, raw = grid_search.fit(folds_without_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure, result in best.items():\n",
    "    parameters = result['parameters']\n",
    "    mean = result['mean']\n",
    "\n",
    "    print(f\"{measure:<30}|  {mean:.3f}  |  {parameters}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "recommender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
