{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation of Recommender Systems\n",
    "\n",
    "This notebook exemplifies how to evaluate the performance of a recommender\n",
    "system using the implementation from evaluation.benchmark module.\n",
    "\n",
    "Copyright 2024 Bernardo C. Rodrigues\n",
    "\n",
    "See COPYING file for license details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup notebook\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluation.plot as plot\n",
    "\n",
    "# Load the autoreload extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# Set autoreload to reload all modules every time a cell is executed\n",
    "%autoreload 2\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset.common import resolve_folds\n",
    "from dataset.movie_lens import load_ml_100k_folds\n",
    "\n",
    "data, k_fold = load_ml_100k_folds()\n",
    "folds = resolve_folds(data, k_fold)\n",
    "\n",
    "folds_without_index = [fold[1] for fold in folds]\n",
    "\n",
    "_, (trainset, testset) = folds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.strategies import (\n",
    "    MAEStrategy,\n",
    "    RMSEStrategy,\n",
    "    MicroAveragedRecallStrategy,\n",
    "    MacroAveragedRecallStrategy,\n",
    "    RecallAtKStrategy,\n",
    "    MicroAveragedPrecisionStrategy,\n",
    "    MacroAveragedPrecisionStrategy,\n",
    "    PrecisionAtKStrategy,\n",
    "    F1ScoreStrategy,\n",
    "    NDCGStrategy,\n",
    "    PredictionCoverageStrategy,\n",
    ")\n",
    "\n",
    "train_measures = [ ]\n",
    "\n",
    "test_measures = [\n",
    "    MAEStrategy(verbose=False),\n",
    "    RMSEStrategy(verbose=False),\n",
    "    MicroAveragedRecallStrategy(threshold=4.0),\n",
    "    MacroAveragedRecallStrategy(threshold=4.0),\n",
    "    RecallAtKStrategy(k=20, threshold=4.0),\n",
    "    MicroAveragedPrecisionStrategy(threshold=4.0),\n",
    "    MacroAveragedPrecisionStrategy(threshold=4.0),\n",
    "    PrecisionAtKStrategy(k=20, threshold=4.0),\n",
    "    F1ScoreStrategy(k=20, threshold=4.0),\n",
    "    NDCGStrategy(k=20, threshold=4.0),\n",
    "    PredictionCoverageStrategy(),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit_and_score will benchmark the recommender system against a single fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.benchmark import fit_and_score\n",
    "from surprise.prediction_algorithms import SVD\n",
    "\n",
    "recommender = SVD()\n",
    "\n",
    "test_measurements, train_measurements, fit_time, test_time = fit_and_score(\n",
    "    recommender_system=recommender,\n",
    "    trainset=trainset,\n",
    "    testset=testset,\n",
    "    test_measures=test_measures,\n",
    "    train_measures=train_measures,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "for measure, measurement in test_measurements.items():\n",
    "    print(f\"{measure:<30}|  {measurement:.3f}\")\n",
    "\n",
    "print(f\"Fit time:                     |  {fit_time:.3f}\")\n",
    "print(f\"Test time:                    |  {test_time:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validate will benchmark the recommender system against multiple folds and return a list of scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.benchmark import cross_validate\n",
    "\n",
    "measurements = cross_validate(\n",
    "    recommender_system=recommender,\n",
    "    folds=folds_without_index,\n",
    "    test_measures=test_measures,\n",
    "    train_measures=train_measures,\n",
    "    max_workers=16,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure, measurement in measurements.items():\n",
    "    print(f\"{measure:<30}|  {measurement}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_validate_recommenders will benchmark the list of recommenders using the provided folds and measures. The function will return a dictionary with the measurements for each recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms import SVD\n",
    "from evaluation.benchmark import cross_validade_recommenders\n",
    "\n",
    "recommenders = [\n",
    "    SVD(n_factors=10, n_epochs=10),\n",
    "    SVD(n_factors=10, n_epochs=20),\n",
    "    SVD(n_factors=20, n_epochs=20),\n",
    "    SVD(n_factors=20, n_epochs=40),\n",
    "    SVD(n_factors=40, n_epochs=40),\n",
    "]\n",
    "\n",
    "recommenders_measurements = cross_validade_recommenders(\n",
    "    recommenders=recommenders,\n",
    "    folds=folds_without_index,\n",
    "    test_measures=test_measures,\n",
    "    train_measures=train_measures,\n",
    "    max_workers=16,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for recommender_measurements in recommenders_measurements:\n",
    "    for measure, measurement in recommender_measurements.items():\n",
    "        print(f\"{measure:<30}|  {measurement}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms import SVD\n",
    "from evaluation.benchmark import GridSearch\n",
    "\n",
    "parameters_grid = {\n",
    "    \"n_factors\": [50, 100, 200],\n",
    "    \"n_epochs\": [10, 20, 40],\n",
    "    \"biased\": [True, False],\n",
    "}\n",
    "\n",
    "grid_search = GridSearch(\n",
    "    SVD,\n",
    "    parameters_grid,\n",
    "    test_measures,\n",
    "    train_measures,\n",
    "    max_workers=16,\n",
    ")\n",
    "\n",
    "best, ordering, raw = grid_search.fit(folds_without_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for measure, result in best.items():\n",
    "    parameters = result['parameters']\n",
    "    mean = result['mean']\n",
    "\n",
    "    print(f\"{measure:<30}|  {mean:.3f}  |  {parameters}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recommender",
   "language": "python",
   "name": "recommender"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
