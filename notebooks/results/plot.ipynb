{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoreload extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# Set autoreload to reload all modules every time a cell is executed\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "DATABASE = sqlite3.connect(\"your_database.db\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize plotly's template\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "DPI = 300\n",
    "WIDTH = 1200\n",
    "HEIGHT = 800\n",
    "FORMAT = \"png\"\n",
    "\n",
    "\n",
    "def customize_default_template():\n",
    "    \"\"\"\n",
    "    Customize the default template with specific layout settings.\n",
    "\n",
    "    This function modifies the default template provided by Plotly with customizations\n",
    "    to the font, margin, width, background color, y-axis, x-axis, and legend.\n",
    "\n",
    "    The modified template is then set as the default template for all subsequent figures.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    # Access the default template\n",
    "    default_template = pio.templates[pio.templates.default]\n",
    "\n",
    "    # Customize font settings\n",
    "    default_template.layout.font.family = \"Latin Modern\"\n",
    "    default_template.layout.font.size = 16\n",
    "    default_template.layout.font.color = \"black\"\n",
    "\n",
    "    # Customize margin and width\n",
    "    default_template.layout.margin = go.layout.Margin(t=50, b=50, l=50, r=50)\n",
    "    default_template.layout.width = WIDTH\n",
    "    default_template.layout.height = HEIGHT\n",
    "\n",
    "    # Customize background color\n",
    "    default_template.layout.plot_bgcolor = \"rgb(245,245,245)\"\n",
    "\n",
    "    # Customize y-axis settings\n",
    "    default_template.layout.yaxis = dict(\n",
    "        mirror=True, ticks=\"outside\", showline=True, linecolor=\"black\", gridcolor=\"lightgrey\"\n",
    "    )\n",
    "\n",
    "    # Customize x-axis settings\n",
    "    default_template.layout.xaxis = dict(\n",
    "        mirror=True, ticks=\"outside\", showline=True, linecolor=\"black\", gridcolor=\"lightgrey\"\n",
    "    )\n",
    "\n",
    "    # Customize legend background color\n",
    "    default_template.layout.legend = dict(bgcolor=\"rgb(245,245,245)\")\n",
    "\n",
    "    # Set the default renderer to JPEG\n",
    "    pio.renderers.default = FORMAT\n",
    "\n",
    "\n",
    "# Call the function to customize the default template\n",
    "customize_default_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an Experiment: Full 100K @ 50K epochs\n",
    "from lib.BinaryDataset import BinaryDataset\n",
    "import pickle\n",
    "\n",
    "binary_dataset = BinaryDataset.load_from_binaps_compatible_input(\"datasets/movielens_100k.dat\")\n",
    "\n",
    "# Set the experiment ID\n",
    "EXPERIMENT_ID = 45\n",
    "\n",
    "# Create a cursor object to interact with the database\n",
    "cursor = DATABASE.cursor()\n",
    "\n",
    "# Fetch the experiment details from the database\n",
    "cursor.execute(\"SELECT * FROM binaps_experiments WHERE id = ?\", (EXPERIMENT_ID,))\n",
    "results = cursor.fetchone()\n",
    "\n",
    "# Unpack the results into individual variables\n",
    "(\n",
    "    id,\n",
    "    dataset,\n",
    "    train_set_size,\n",
    "    batch_size,\n",
    "    test_batch_size,\n",
    "    epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    gamma,\n",
    "    seed,\n",
    "    hidden_dimension,\n",
    "    serialized_weights,\n",
    "    serialized_training_losses,\n",
    "    serialized_test_losses,\n",
    "    runtime,\n",
    ") = results\n",
    "\n",
    "# Deserialize the weights, training losses, and test losses from their serialized forms\n",
    "weights = pickle.loads(serialized_weights)\n",
    "training_losses = pickle.loads(serialized_training_losses)\n",
    "test_losses = pickle.loads(serialized_test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Runtime\n",
    "import datetime\n",
    "\n",
    "print(runtime)\n",
    "str(datetime.timedelta(seconds=runtime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Loss convergence\n",
    "\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Generate x-values\n",
    "x = np.arange(len(test_losses))\n",
    "\n",
    "# Calculate LOWESS trendline\n",
    "lowess_data = lowess(test_losses, x, frac=0.1)\n",
    "lowess_x = lowess_data[:, 0]\n",
    "lowess_y = lowess_data[:, 1]\n",
    "\n",
    "\n",
    "# Calculate cumulative minimum trendline\n",
    "cumulative_min = np.minimum.accumulate(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the scatter plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add scatter trace\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=test_losses,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"black\", size=2, opacity=0.2),\n",
    "        name=\"Test Loss\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add LOWESS trendline trace\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=lowess_x,\n",
    "        y=lowess_y,\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"cyan\", dash=\"dash\"),\n",
    "        name=\"LOWESS Trendline\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add cumulative minimum trendline trace\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=cumulative_min,\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"red\", dash=\"dot\"),\n",
    "        name=\"Cumulative Minimum\",\n",
    "    )\n",
    ")\n",
    "\n",
    "# Update axis labels\n",
    "fig.update_layout(xaxis=dict(title=\"Epochs\"), yaxis=dict(title=\"Loss\"))\n",
    "\n",
    "# Update the legend title\n",
    "fig.update_traces(showlegend=True)\n",
    "fig.update_layout(\n",
    "    width=WIDTH,\n",
    "    height=HEIGHT / 2,\n",
    "    margin_l=60,\n",
    ")\n",
    "fig.show(renderer=\"png\")\n",
    "\n",
    "fig.write_image(\"loss.png\", format=FORMAT, width=WIDTH, height=HEIGHT / 2, scale=DPI / 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Binaps formal context\n",
    "\n",
    "from fca.FormalConceptAnalysis import construct_context_from_binaps_patterns\n",
    "from lib.BinapsWrapper import get_patterns_from_weights\n",
    "\n",
    "patterns = get_patterns_from_weights(weights, 0.2)\n",
    "\n",
    "# Construct the context from the binaps patterns\n",
    "binaps_context = construct_context_from_binaps_patterns(binary_dataset, patterns, True)\n",
    "\n",
    "print(len(binaps_context))\n",
    "\n",
    "binaps_intent_sizes = []\n",
    "binaps_extent_sizes = []\n",
    "binaps_concept_sizes = []\n",
    "\n",
    "# Iterate over each formal concept in the context\n",
    "for formal_concept in binaps_context:\n",
    "    # Calculate the sizes of the formal concept's intent and extent\n",
    "    formal_concept_intent_size = len(formal_concept.intent)\n",
    "    formal_concept_extent_size = len(formal_concept.extent)\n",
    "\n",
    "    # Append the intent and extent sizes to the respective lists\n",
    "    binaps_intent_sizes.append(formal_concept_intent_size)\n",
    "    binaps_extent_sizes.append(formal_concept_extent_size)\n",
    "\n",
    "    # Calculate the concept size as the product of intent size and extent size, divided by 20\n",
    "    binaps_concept_sizes.append(formal_concept_intent_size * formal_concept_extent_size / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate GreCond formal context\n",
    "\n",
    "# import fca\n",
    "from fca.FormalConceptAnalysis import GreConD\n",
    "\n",
    "grecond_formal_context, _ = GreConD(binary_dataset, coverage=1)\n",
    "\n",
    "# Print the length of the formal context\n",
    "print(len(grecond_formal_context))\n",
    "\n",
    "# Initialize empty lists to store concept sizes\n",
    "grecond_intent_sizes = []\n",
    "grecond_extent_sizes = []\n",
    "grecond_concept_sizes = []\n",
    "\n",
    "# Iterate over the formal concepts in the formal context\n",
    "for formal_concept in grecond_formal_context:\n",
    "    # Calculate the intent and extent sizes\n",
    "    formal_concept_intent_size = len(formal_concept.intent)\n",
    "    formal_concept_extent_size = len(formal_concept.extent)\n",
    "\n",
    "    # Append the sizes to the respective lists\n",
    "    grecond_intent_sizes.append(formal_concept_intent_size)\n",
    "    grecond_extent_sizes.append(formal_concept_extent_size)\n",
    "    grecond_concept_sizes.append(formal_concept_intent_size * formal_concept_extent_size / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Binaps and Grecond concepts\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# BinaPs scatter plot\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(\n",
    "    go.Scatter(\n",
    "        x=binaps_intent_sizes,\n",
    "        y=binaps_extent_sizes,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"black\", size=10, opacity=0.5),\n",
    "        name=\"Concept Area\",\n",
    "    )\n",
    ")\n",
    "fig1.update_layout(\n",
    "    xaxis_title=\"Concept Intent Size\",\n",
    "    yaxis_title=\"Concept Extent Size\",\n",
    "    width=500,\n",
    "    height=500,\n",
    "    showlegend=False,\n",
    "    xaxis=dict(range=[-100, 800], tickvals=[0, 200, 400, 600, 800]),\n",
    "    yaxis=dict(range=[-100, 800], tickvals=[0, 200, 400, 600, 800]),\n",
    "    margin_l=80,\n",
    ")\n",
    "\n",
    "# GreCond2 scatter plot\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(\n",
    "    go.Scatter(\n",
    "        x=grecond_intent_sizes,\n",
    "        y=grecond_extent_sizes,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(\n",
    "            color=\"black\",\n",
    "            opacity=0.5,\n",
    "            size=10,\n",
    "        ),\n",
    "        name=\"Concept\",\n",
    "    )\n",
    ")\n",
    "fig2.update_layout(\n",
    "    xaxis_title=\"Concept Intent Size\",\n",
    "    yaxis_title=\"Concept Extent Size\",\n",
    "    width=575,\n",
    "    height=500,\n",
    "    showlegend=True,\n",
    "    xaxis=dict(range=[-100, 800], tickvals=[0, 200, 400, 600, 800]),\n",
    "    yaxis=dict(range=[-100, 800], tickvals=[0, 200, 400, 600, 800]),\n",
    "    margin_l=80,\n",
    ")\n",
    "\n",
    "\n",
    "# Show the figures\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "\n",
    "fig1.write_image(\n",
    "    \"concepts_binaps.png\", format=FORMAT, width=HEIGHT / 2, height=HEIGHT / 2, scale=DPI / 96\n",
    ")\n",
    "fig2.write_image(\"concepts_grecond.png\", format=FORMAT, width=575, height=500, scale=DPI / 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# boxplots for Binaps and Grecond concepts\n",
    "\n",
    "fig1 = go.Figure()\n",
    "fig1.add_trace(\n",
    "    go.Box(y=binaps_intent_sizes, name=\"BinaPs\", marker_color=\"black\"),\n",
    ")\n",
    "fig1.add_trace(go.Box(y=grecond_intent_sizes, name=\"GreConD\"))\n",
    "\n",
    "fig2 = go.Figure()\n",
    "fig2.add_trace(\n",
    "    go.Box(y=binaps_extent_sizes, name=\"BinaPs\", marker_color=\"black\"),\n",
    ")\n",
    "fig2.add_trace(go.Box(y=grecond_extent_sizes,  name=\"GreConD\"))\n",
    "\n",
    "fig3 = go.Figure()\n",
    "fig3.add_trace(\n",
    "    go.Box(y=binaps_concept_sizes, name=\"BinaPs\", marker_color=\"black\"),\n",
    ")\n",
    "fig3.add_trace(go.Box(y=grecond_concept_sizes, name=\"GreConD\"))\n",
    "\n",
    "\n",
    "fig1.update_layout(width=WIDTH, height=HEIGHT / 2, yaxis_title=\"Concept Intent Size\", margin_l=80)\n",
    "fig2.update_layout(width=WIDTH, height=HEIGHT / 2, yaxis_title=\"Concept Extent Size\", margin_l=80)\n",
    "fig3.update_layout(width=WIDTH, height=HEIGHT / 2, yaxis_title=\"Concept Area\", margin_l=80)\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()\n",
    "fig3.show()\n",
    "\n",
    "fig1.write_image(\"intent_size.png\", format=FORMAT, width=WIDTH, height=HEIGHT / 2, scale=DPI / 96)\n",
    "fig2.write_image(\"extent_size.png\", format=FORMAT, width=WIDTH, height=HEIGHT / 2, scale=DPI / 96)\n",
    "fig3.write_image(\"area_size.png\", format=FORMAT, width=WIDTH, height=HEIGHT / 2, scale=DPI / 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the latent space matrices\n",
    "\n",
    "from fca.FormalConceptAnalysis import get_factor_matrices_from_concepts\n",
    "\n",
    "binaps_Af, binaps_Bf = get_factor_matrices_from_concepts(\n",
    "    binaps_context, binary_dataset.shape[0], binary_dataset.shape[1]\n",
    ")\n",
    "\n",
    "grecond_Af, _ = get_factor_matrices_from_concepts(\n",
    "    grecond_formal_context, binary_dataset.shape[0], binary_dataset.shape[1]\n",
    ")\n",
    "\n",
    "I = np.matmul(binaps_Af, binaps_Bf)\n",
    "\n",
    "real_coverage = np.count_nonzero(I) / np.count_nonzero(binary_dataset._binary_dataset)\n",
    "print(real_coverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose cosine as the similarity distance strategy\n",
    "\n",
    "from lib.BooleanMatrixBasedRecomenders import cosine_distance as distance_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the similarities matrices\n",
    "\n",
    "from lib.BooleanMatrixBasedRecomenders import get_similarity_matrix\n",
    "\n",
    "binary_dataset_similarities = get_similarity_matrix(binary_dataset, distance_strategy)\n",
    "\n",
    "# Binaps Similarity matrix\n",
    "binaps_latent_binary_dataset = BinaryDataset(binaps_Af)\n",
    "binaps_latent_binary_dataset_similarities = get_similarity_matrix(\n",
    "    binaps_latent_binary_dataset, distance_strategy\n",
    ")\n",
    "binaps_similarity_delta = binaps_latent_binary_dataset_similarities - binary_dataset_similarities\n",
    "\n",
    "# # Grecond Similarity matrix\n",
    "grecond_latent_binary_dataset = BinaryDataset(grecond_Af)\n",
    "grecond_latent_binary_dataset_similarities = get_similarity_matrix(\n",
    "    grecond_latent_binary_dataset, distance_strategy\n",
    ")\n",
    "grecond_similarity_delta = grecond_latent_binary_dataset_similarities - binary_dataset_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot all similarity matrices\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "def plot_similarities(binary_dataset_similarities, colorscale, delta_range, file_name):\n",
    "    fig = go.Figure(\n",
    "        data=go.Heatmap(\n",
    "            z=binary_dataset_similarities,\n",
    "            colorbar=dict(\n",
    "                title=\"Similarity\",\n",
    "                titleside=\"right\",\n",
    "            ),\n",
    "            colorscale=colorscale,\n",
    "            zmin=delta_range[0],\n",
    "            zmax=delta_range[1],\n",
    "        ),\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        xaxis_title=\"User\",\n",
    "        yaxis_title=\"User\",\n",
    "        width=470,\n",
    "        height=400,\n",
    "        margin_l=60,\n",
    "    )\n",
    "    fig.update_yaxes(autorange=\"reversed\")\n",
    "    fig.show()\n",
    "    fig.write_image(file_name, format=FORMAT, scale=DPI / 96)\n",
    "\n",
    "similary_colorscale = [\n",
    "    [0.0, \"blue\"],\n",
    "    [0.25, \"cyan\"],\n",
    "    [0.5, \"green\"],\n",
    "    [0.75, \"yellow\"],\n",
    "    [1.0, \"red\"],\n",
    "]\n",
    "\n",
    "plot_similarities(\n",
    "    binary_dataset_similarities, similary_colorscale, (0, 1), f\"dataset_similarities_{distance_strategy.__name__}.png\"\n",
    ")\n",
    "\n",
    "plot_similarities(\n",
    "    binaps_latent_binary_dataset_similarities,\n",
    "    similary_colorscale,\n",
    "    (0, 1),\n",
    "    f\"binaps_latent_binary_dataset_similarities_{distance_strategy.__name__}.png\",\n",
    ")\n",
    "plot_similarities(\n",
    "    binaps_similarity_delta, similary_colorscale, (-0.15, 0.15), f\"binaps_similarity_delta_{distance_strategy.__name__}.png\"\n",
    ")\n",
    "\n",
    "plot_similarities(\n",
    "    grecond_latent_binary_dataset_similarities,\n",
    "    similary_colorscale,\n",
    "    (0, 1),\n",
    "    f\"grecond_latent_binary_dataset_similarities_{distance_strategy.__name__}.png\",\n",
    ")\n",
    "plot_similarities(\n",
    "    grecond_similarity_delta, similary_colorscale, (-0.15, 0.15), f\"grecond_similarity_delta_{distance_strategy.__name__}.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute neighbor similarities correlations\n",
    "\n",
    "from statistics import mean\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from lib.BooleanMatrixBasedRecomenders import get_k_nearest_neighbors\n",
    "\n",
    "\n",
    "def get_jaccard_distance_from_sets(A, B):\n",
    "    set_a = set(A)\n",
    "    set_b = set(B)\n",
    "\n",
    "    jaccard_similarity = len(set_a.intersection(set_b)) / len(set_a.union(set_b))\n",
    "\n",
    "    return 1 - jaccard_similarity\n",
    "\n",
    "\n",
    "def get_correlations_between_neighbors(ks, neighbors_a, neighbors_b):\n",
    "    spearman_correlations_per_k = []\n",
    "    kendall_correlations_per_k = []\n",
    "    jaccards_per_k = []\n",
    "\n",
    "    for k in ks:\n",
    "        spearman_correlations = []\n",
    "        kendall_correlations = []\n",
    "        jaccards = []\n",
    "\n",
    "        for index in range(binary_dataset_similarities.shape[0]):\n",
    "            a_rankings = get_k_nearest_neighbors(neighbors_a, index, k)\n",
    "            b_rankings = get_k_nearest_neighbors(neighbors_b, index, k)\n",
    "\n",
    "            if not a_rankings.any() or not b_rankings.any():\n",
    "                # print(1)\n",
    "                continue\n",
    "\n",
    "            spearman_correlation, _ = spearmanr(a_rankings, b_rankings)\n",
    "            kendall_correlation, _ = kendalltau(a_rankings, b_rankings)\n",
    "            this_jaccard_distance = get_jaccard_distance_from_sets(a_rankings, b_rankings)\n",
    "\n",
    "            spearman_correlations.append(spearman_correlation)\n",
    "            kendall_correlations.append(kendall_correlation)\n",
    "            jaccards.append(this_jaccard_distance)\n",
    "\n",
    "        spearman_correlations_per_k.append(mean(spearman_correlations))\n",
    "        kendall_correlations_per_k.append(mean(kendall_correlations))\n",
    "        jaccards_per_k.append(mean(jaccards))\n",
    "\n",
    "    return spearman_correlations_per_k, kendall_correlations_per_k, jaccards_per_k\n",
    "\n",
    "\n",
    "ks = list(range(1, 100))\n",
    "\n",
    "\n",
    "(\n",
    "    binaps_spearman_correlations_per_k,\n",
    "    binaps_kendall_correlations_per_k,\n",
    "    binaps_jaccards_per_k,\n",
    ") = get_correlations_between_neighbors(\n",
    "    ks, binary_dataset_similarities, binaps_latent_binary_dataset_similarities\n",
    ")\n",
    "\n",
    "(\n",
    "    grecond_spearman_correlations_per_k,\n",
    "    grecond_kendall_correlations_per_k,\n",
    "    grecond_jaccards_per_k,\n",
    ") = get_correlations_between_neighbors(\n",
    "    ks, binary_dataset_similarities, grecond_latent_binary_dataset_similarities\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot neighbor similarities correlations\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ks,\n",
    "        y=grecond_spearman_correlations_per_k,\n",
    "        name=\"Mean Spearman Correlation\",\n",
    "        legendgroup=\"group\",\n",
    "        legendgrouptitle_text=\"GreCond\",\n",
    "        line=dict(color=\"blue\", width=4),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ks,\n",
    "        y=binaps_spearman_correlations_per_k,\n",
    "        name=\"Mean Spearman Correlation\",\n",
    "        legendgroup=\"group2\",\n",
    "        legendgrouptitle_text=\"BinaPs\",\n",
    "        line=dict(color=\"blue\", dash=\"dot\", width=4),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ks,\n",
    "        y=grecond_kendall_correlations_per_k,\n",
    "        name=\"Mean Kendall Correlation\",\n",
    "        legendgroup=\"group\",\n",
    "        line=dict(color=\"green\", width=4),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ks,\n",
    "        y=binaps_kendall_correlations_per_k,\n",
    "        name=\"Mean Kendall Correlation\",\n",
    "        legendgroup=\"group2\",\n",
    "        line=dict(color=\"green\", dash=\"dot\", width=4),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ks,\n",
    "        y=grecond_jaccards_per_k,\n",
    "        name=\"Mean Jaccard Distance\",\n",
    "        legendgroup=\"group\",\n",
    "        line=dict(color=\"red\", width=4),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=ks,\n",
    "        y=binaps_jaccards_per_k,\n",
    "        name=\"Mean Jaccard Distance\",\n",
    "        legendgroup=\"group2\",\n",
    "        line=dict(color=\"red\", dash=\"dot\", width=4),\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Update the legend title\n",
    "fig.update_traces(showlegend=True)\n",
    "fig.update_layout(height=HEIGHT / 2)\n",
    "fig.update_xaxes(title=\"K nearest neighbors\")\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"top_k_correlation.png\", format=FORMAT, scale=DPI / 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folds from movie lens 100k\n",
    "\n",
    "import os\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import PredefinedKFold\n",
    "\n",
    "\n",
    "fold_files_dir = os.path.expanduser(\"/workdir/datasets/ml-100k\")\n",
    "folds_files = [(f\"{fold_files_dir}/u{i}.base\", f\"{fold_files_dir}/u{i}.test\") for i in (1, 2, 3, 4, 5)]\n",
    "\n",
    "reader = Reader(\"ml-100k\")\n",
    "dataset = Dataset.load_from_folds(folds_files, reader=reader)\n",
    "pkf = PredefinedKFold()\n",
    "\n",
    "folds = []\n",
    "for index, (trainset, testset) in enumerate(pkf.split(dataset)):\n",
    "    folds.append((index, trainset, BinaryDataset.load_from_trainset(trainset), testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load range of experiments\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from lib.BooleanMatrixBasedRecomenders import BinapsRecommender, cosine_distance\n",
    "from fca.FormalConceptAnalysis import construct_context_from_binaps_patterns\n",
    "from lib.BinapsWrapper import get_patterns_from_weights\n",
    "\n",
    "EPOCH_START_EXPERIMENT_IDS = [56, 61, 66, 71, 76, 81, 86]\n",
    "\n",
    "# experiments = []\n",
    "\n",
    "experiment_predictions_per_epoch = []\n",
    "number_of_patterns_per_epoch = []\n",
    "real_coverages_per_epoch = []\n",
    "\n",
    "for epoch_start_id in EPOCH_START_EXPERIMENT_IDS:\n",
    "\n",
    "    fold_ids = [epoch_start_id + i for i in range(5)]\n",
    "\n",
    "    experiment_predictions = []\n",
    "    number_of_patterns = []\n",
    "    real_coverages = []\n",
    "\n",
    "    for id, (index, trainset, fold_binary_dataset, testset) in zip(fold_ids, folds):\n",
    "\n",
    "        cursor = DATABASE.cursor()\n",
    "        cursor.execute(\"SELECT * FROM binaps_experiments WHERE id = ?\", (id,))\n",
    "        (\n",
    "            id,\n",
    "            dataset,\n",
    "            train_set_size,\n",
    "            batch_size,\n",
    "            test_batch_size,\n",
    "            epochs,\n",
    "            learning_rate,\n",
    "            weight_decay,\n",
    "            gamma,\n",
    "            seed,\n",
    "            hidden_dimension,\n",
    "            serialized_weights,\n",
    "            serialized_training_losses,\n",
    "            serialized_test_losses,\n",
    "            runtime,\n",
    "        ) = cursor.fetchone()\n",
    "\n",
    "        weights = pickle.loads(serialized_weights)\n",
    "        training_losses = pickle.loads(serialized_training_losses)\n",
    "        test_losses = pickle.loads(serialized_test_losses)\n",
    "\n",
    "        patterns = get_patterns_from_weights(weights, 0.4)\n",
    "        number_of_patterns.append(len(patterns))\n",
    "\n",
    "        context = construct_context_from_binaps_patterns(fold_binary_dataset, patterns, True)\n",
    "        Af, Bf = get_factor_matrices_from_concepts(\n",
    "            context, fold_binary_dataset.shape[0], fold_binary_dataset.shape[1]\n",
    "        )\n",
    "        I = np.matmul(Af, Bf)\n",
    "\n",
    "        assert np.all(fold_binary_dataset._binary_dataset[I])\n",
    "\n",
    "        real_coverage = np.count_nonzero(I) / np.count_nonzero(binary_dataset._binary_dataset)\n",
    "        real_coverages.append(real_coverage)\n",
    "\n",
    "        recommender = BinapsRecommender.from_previously_computed_patterns(patterns, k=60, threshold=1, distance_strategy=cosine_distance)\n",
    "        recommender.fit(trainset)\n",
    "\n",
    "        fold_predictions = recommender.test(testset)\n",
    "        experiment_predictions.append(fold_predictions)\n",
    "\n",
    "    number_of_patterns_per_epoch.append(mean(number_of_patterns))\n",
    "    real_coverages_per_epoch.append(mean(real_coverages))\n",
    "    experiment_predictions_per_epoch.append(experiment_predictions)\n",
    "\n",
    "\n",
    "print(number_of_patterns_per_epoch)\n",
    "print(real_coverages_per_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pruned_NUM = []\n",
    "\n",
    "def prune_impossible_predictions(predictions):\n",
    "    pruned = [prediction for prediction in predictions if not prediction[4][\"was_impossible\"]]\n",
    "    # pruned_NUM.append(len(pruned))\n",
    "    return pruned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from surprise.accuracy import mae, rmse\n",
    "\n",
    "mae_array = []\n",
    "rmse_array = []\n",
    "\n",
    "\n",
    "for experiment_predictions in experiment_predictions_per_epoch:\n",
    "    maes = []\n",
    "    rmses = []\n",
    "\n",
    "    for fold_predictions in experiment_predictions:\n",
    "        maes.append(\n",
    "            mae(\n",
    "                predictions=prune_impossible_predictions(fold_predictions),\n",
    "                verbose=False,\n",
    "            )\n",
    "        )\n",
    "        rmses.append(\n",
    "            rmse(\n",
    "                predictions=prune_impossible_predictions(fold_predictions),\n",
    "                verbose=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "\n",
    "    mae_array.append(statistics.mean(maes))\n",
    "    rmse_array.append(statistics.mean(rmses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(pruned_NUM)/ 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig1 = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "x_array = [500, 1000, 2000, 5000, 10000, 30000, 50000]\n",
    "\n",
    "fig1.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_array,\n",
    "        y=mae_array,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"MAE\",\n",
    "        line=dict(color=\"blue\", width=4, dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "fig1.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_array,\n",
    "        y=rmse_array,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"RMSE\",\n",
    "        line=dict(color=\"red\", width=4),\n",
    "    ),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "fig1.update_xaxes(title_text=\"Epochs\")\n",
    "fig1.update_yaxes(title_text=\"MAE\", secondary_y=False)\n",
    "fig1.update_yaxes(title_text=\"RMSE\", secondary_y=True)\n",
    "fig1.update_layout(margin_l=100)\n",
    "\n",
    "fig1.update_layout(\n",
    "    yaxis=dict(gridcolor=\"lightblue\"), yaxis2=dict(gridcolor=\"pink\"), height=HEIGHT / 2\n",
    ")\n",
    "\n",
    "fig1.show()\n",
    "\n",
    "fig1.write_image(\"mae_rmse_epochs.png\", format=FORMAT, scale=DPI / 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_array,\n",
    "        y=number_of_patterns_per_epoch,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Number of mined Concepts\",\n",
    "        yaxis=\"y2\",\n",
    "        line=dict(\n",
    "            color=\"green\",\n",
    "            width=4,\n",
    "        ),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_array,\n",
    "        y=real_coverages_per_epoch,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Original matrix reconstruction (%)\",\n",
    "        yaxis=\"y\",\n",
    "        line=dict(color=\"blue\", width=4, dash=\"dot\"),\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x_array,\n",
    "        y=rmse_array,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"RMSE\",\n",
    "        yaxis=\"y3\",\n",
    "        line=dict(color=\"red\", width=4, dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(domain=[0.15, 0.95]),\n",
    "    yaxis=dict(\n",
    "        title=\"Original matrix reconstruction (%)\",\n",
    "        tickformat=\".0%\",\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"Number of mined Concepts\",\n",
    "        anchor=\"free\",\n",
    "        overlaying=\"y\",\n",
    "        side=\"left\",\n",
    "        position=0.05,\n",
    "    ),\n",
    "    yaxis3=dict(\n",
    "        title=\"RMSE\",\n",
    "        anchor=\"x\",\n",
    "        overlaying=\"y\",\n",
    "        side=\"right\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Epochs\")\n",
    "\n",
    "fig.update_layout(\n",
    "    yaxis=dict(gridcolor=\"lightblue\"),\n",
    "    yaxis2=dict(gridcolor=\"lightgreen\"),\n",
    "    yaxis3=dict(gridcolor=\"pink\"),\n",
    "    height=HEIGHT / 2,\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"reconstruction_x_epochs.png\", format=FORMAT, scale=DPI / 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run binaps\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load range of experiments\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from lib.BooleanMatrixBasedRecomenders import BinapsRecommender, cosine_distance\n",
    "from fca.FormalConceptAnalysis import construct_context_from_binaps_patterns\n",
    "from lib.BinapsWrapper import get_patterns_from_weights\n",
    "\n",
    "EPOCH_START_EXPERIMENT_IDS = [56, 61, 66, 71, 76, 81, 86]\n",
    "\n",
    "ks = [1, 5, 10, 20, 30, 40, 50, 60]\n",
    "\n",
    "x_array = []\n",
    "y_array = []\n",
    "z_array = []\n",
    "\n",
    "for epoch_start_id in EPOCH_START_EXPERIMENT_IDS:\n",
    "\n",
    "    fold_ids = [epoch_start_id + i for i in range(5)]\n",
    "\n",
    "    # experiment_predictions = []\n",
    "    # number_of_patterns = []\n",
    "    # real_coverages = []\n",
    "\n",
    "    for k in ks:\n",
    "        recommender.k = k\n",
    "\n",
    "        experiment_rmse = []\n",
    "\n",
    "        for id, (index, trainset, fold_binary_dataset, testset) in zip(fold_ids, folds):\n",
    "\n",
    "            cursor = DATABASE.cursor()\n",
    "            cursor.execute(\"SELECT * FROM binaps_experiments WHERE id = ?\", (id,))\n",
    "            (\n",
    "                id,\n",
    "                dataset,\n",
    "                train_set_size,\n",
    "                batch_size,\n",
    "                test_batch_size,\n",
    "                epochs,\n",
    "                learning_rate,\n",
    "                weight_decay,\n",
    "                gamma,\n",
    "                seed,\n",
    "                hidden_dimension,\n",
    "                serialized_weights,\n",
    "                serialized_training_losses,\n",
    "                serialized_test_losses,\n",
    "                runtime,\n",
    "            ) = cursor.fetchone()\n",
    "\n",
    "            weights = pickle.loads(serialized_weights)\n",
    "            training_losses = pickle.loads(serialized_training_losses)\n",
    "            test_losses = pickle.loads(serialized_test_losses)\n",
    "\n",
    "            patterns = get_patterns_from_weights(weights, 0.7)\n",
    "\n",
    "            recommender = BinapsRecommender.from_previously_computed_patterns(patterns, k=k, threshold=1, distance_strategy=cosine_distance)\n",
    "            recommender.fit(trainset)\n",
    "\n",
    "            fold_predictions = recommender.test(testset)\n",
    "            fold_rmse = rmse(predictions=fold_predictions)\n",
    "            experiment_rmse.append(fold_rmse)\n",
    "\n",
    "        x_array.append(epochs)\n",
    "        y_array.append(k)\n",
    "        z_array.append(statistics.mean(experiment_rmse))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Mesh3d(\n",
    "            x=x_array,\n",
    "            y=y_array,\n",
    "            z=z_array,\n",
    "            # opacity=0.6,\n",
    "            intensity=z_array,\n",
    "            # colorscale=\"Viridis\",\n",
    "            colorscale=[[0, \"cyan\"], [0.05, \"blue\"], [1.0, \"purple\"]],\n",
    "            colorbar_title=\"RMSE\",\n",
    "        ),\n",
    "        go.Scatter3d(\n",
    "            x=x_array,\n",
    "            y=y_array,\n",
    "            z=z_array,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=4, color=\"yellow\"),\n",
    "            hoverinfo=\"none\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis_title=\"Epochs\",\n",
    "        yaxis_title=\"K\",\n",
    "        zaxis_title=\"RMSE\",\n",
    "        # xaxis=dict(autorange=\"reversed\"),  # Reversing the x-axis\n",
    "        yaxis=dict(autorange=\"reversed\"),  # Reversing the x-axis\n",
    "    ),\n",
    "    width=500,\n",
    "    height=HEIGHT / 2,\n",
    "    scene_camera=dict(\n",
    "        up=dict(x=0, y=0, z=1), center=dict(x=-0.2, y=0, z=-0.1), eye=dict(x=-1.25, y=-1., z=1.5)\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig.update_yaxes(autorange=\"reversed\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"rmse_epochs_k.png\", format=FORMAT, scale=DPI / 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run binaps\n",
    "\n",
    "\n",
    "# Load range of experiments\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from lib.BooleanMatrixBasedRecomenders import BinapsRecommender, cosine_distance\n",
    "from fca.FormalConceptAnalysis import construct_context_from_binaps_patterns\n",
    "from lib.BinapsWrapper import get_patterns_from_weights\n",
    "\n",
    "EPOCH_START_EXPERIMENT_IDS = [56, 61, 66, 71, 76, 81, 86]\n",
    "\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "x_array = []\n",
    "y_array = []\n",
    "z_array = []\n",
    "\n",
    "for epoch_start_id in EPOCH_START_EXPERIMENT_IDS:\n",
    "\n",
    "    fold_ids = [epoch_start_id + i for i in range(5)]\n",
    "\n",
    "    for threshold in thresholds:\n",
    "\n",
    "        experiment_rmse = []\n",
    "\n",
    "        for id, (index, trainset, fold_binary_dataset, testset) in zip(fold_ids, folds):\n",
    "\n",
    "            cursor = DATABASE.cursor()\n",
    "            cursor.execute(\"SELECT * FROM binaps_experiments WHERE id = ?\", (id,))\n",
    "            (\n",
    "                id,\n",
    "                dataset,\n",
    "                train_set_size,\n",
    "                batch_size,\n",
    "                test_batch_size,\n",
    "                epochs,\n",
    "                learning_rate,\n",
    "                weight_decay,\n",
    "                gamma,\n",
    "                seed,\n",
    "                hidden_dimension,\n",
    "                serialized_weights,\n",
    "                serialized_training_losses,\n",
    "                serialized_test_losses,\n",
    "                runtime,\n",
    "            ) = cursor.fetchone()\n",
    "\n",
    "            weights = pickle.loads(serialized_weights)\n",
    "            training_losses = pickle.loads(serialized_training_losses)\n",
    "            test_losses = pickle.loads(serialized_test_losses)\n",
    "\n",
    "            patterns = get_patterns_from_weights(weights, threshold)\n",
    "\n",
    "            recommender = BinapsRecommender.from_previously_computed_patterns(patterns, k=60, threshold=1, distance_strategy=cosine_distance)\n",
    "            recommender.fit(trainset)\n",
    "\n",
    "            fold_predictions = recommender.test(testset)\n",
    "            fold_rmse = rmse(predictions=fold_predictions)\n",
    "            experiment_rmse.append(fold_rmse)\n",
    "\n",
    "        x_array.append(epochs)\n",
    "        y_array.append(threshold)\n",
    "        z_array.append(statistics.mean(experiment_rmse))\n",
    "\n",
    "\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# from lib.BooleanMatrixBasedRecomenders import BinapsRecommender\n",
    "# from lib.BinapsWrapper import get_patterns_from_weights\n",
    "\n",
    "# x = 5  # epochs\n",
    "# x_array = []\n",
    "# y_array = []\n",
    "# z_array = []\n",
    "\n",
    "# predictions = []\n",
    "# number_of_patterns = []\n",
    "# real_coverages = []\n",
    "\n",
    "# thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "\n",
    "# for experiment in experiments:\n",
    "#     (\n",
    "#         id,\n",
    "#         dataset,\n",
    "#         train_set_size,\n",
    "#         batch_size,\n",
    "#         test_batch_size,\n",
    "#         epochs,\n",
    "#         learning_rate,\n",
    "#         weight_decay,\n",
    "#         gamma,\n",
    "#         seed,\n",
    "#         hidden_dimension,\n",
    "#         serialized_weights,\n",
    "#         serialized_training_losses,\n",
    "#         serialized_test_losses,\n",
    "#         runtime,\n",
    "#     ) = experiment\n",
    "\n",
    "#     weights = pickle.loads(serialized_weights)\n",
    "#     training_losses = pickle.loads(serialized_training_losses)\n",
    "#     test_losses = pickle.loads(serialized_test_losses)\n",
    "\n",
    "#     for threshold in thresholds:\n",
    "#         patterns = get_patterns_from_weights(weights, threshold)\n",
    "\n",
    "#         recommender = BinapsRecommender.from_previously_computed_patterns(\n",
    "#             patterns, k=60, threshold=1\n",
    "#         )\n",
    "\n",
    "#         experiment_rmse = []\n",
    "\n",
    "#         for index, trainset, testset in folds:\n",
    "#             recommender.fit(trainset)\n",
    "\n",
    "#             fold_predictions = recommender.test(testset)\n",
    "#             fold_rmse = rmse(predictions=fold_predictions)\n",
    "#             experiment_rmse.append(fold_rmse)\n",
    "\n",
    "#         x_array.append(experiment[x])\n",
    "#         y_array.append(threshold)\n",
    "#         z_array.append(statistics.mean(experiment_rmse))\n",
    "\n",
    "#     # predictions.append(experiment_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Mesh3d(\n",
    "            x=x_array,\n",
    "            y=y_array,\n",
    "            z=z_array,\n",
    "            opacity=0.9,\n",
    "            intensity=z_array,\n",
    "            colorscale=[[0, \"cyan\"], [0.1, \"blue\"], [1.0, \"purple\"]],\n",
    "            colorbar_title=\"RMSE\",\n",
    "        ),\n",
    "        go.Scatter3d(\n",
    "            x=x_array,\n",
    "            y=y_array,\n",
    "            z=z_array,\n",
    "            mode=\"markers\",\n",
    "            marker=dict(size=4, color=\"yellow\"),  # Set the marker size  # Set the marker color\n",
    "            hoverinfo=\"none\",  # Disable hoverinfo for the markers\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(xaxis_title=\"Epochs\", yaxis_title=\"Binarization Threshold\", zaxis_title=\"RMSE\")\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            title=\"Epochs\",\n",
    "            tickmode=\"array\",  # Use specific tick values\n",
    "            tickvals=[0, 25000, 50000],  # Specify the desired tick values for the x-axis\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"Binarization<br>Threshold\",\n",
    "            tickmode=\"array\",  # Use specific tick values\n",
    "            tickvals=[\n",
    "                0,\n",
    "                0.2,\n",
    "                0.4,\n",
    "                0.6,\n",
    "                0.8,\n",
    "            ],  # Specify the desired tick values for the x-axis\n",
    "            autorange=\"reversed\",\n",
    "        ),\n",
    "        zaxis_title=\"RMSE\",\n",
    "        # xaxis=dict(autorange=\"reversed\"),  # Reversing the x-axis\n",
    "    ),\n",
    "    width=500,\n",
    "    height=HEIGHT / 2,\n",
    "    scene_camera=dict(\n",
    "        up=dict(x=0, y=0, z=1),\n",
    "        center=dict(x=0.1, y=0, z=-0.3),\n",
    "        eye=dict(x=-1.3625, y=-1.3625, z=1.1),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# fig.update_yaxes(autorange=\"reversed\")/\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fig.write_image(\"rmse_epochs_threshold.png\", format=FORMAT, scale=DPI / 96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a trainset from the complemte MovieLens 100K dataset\n",
    "from surprise import Dataset\n",
    "\n",
    "dataset = Dataset.load_builtin(\"ml-100k\")\n",
    "trainset = dataset.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import os\n",
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import PredefinedKFold\n",
    "from lib.BooleanMatrixBasedRecomenders import cosine_distance, jaccard_distance, BinaryDataset\n",
    "\n",
    "fold_files_dir = os.path.expanduser(\"/workdir/datasets/ml-100k\")\n",
    "folds_files = [(f\"{fold_files_dir}/u{i}.base\", f\"{fold_files_dir}/u{i}.test\") for i in (1, 2, 3, 4, 5)]\n",
    "\n",
    "reader = Reader(\"ml-100k\")\n",
    "dataset = Dataset.load_from_folds(folds_files, reader=reader)\n",
    "pkf = PredefinedKFold()\n",
    "\n",
    "folds = []\n",
    "for index, (trainset, testset) in enumerate(pkf.split(dataset)):\n",
    "    folds.append((index, trainset, BinaryDataset.load_from_trainset(trainset), testset))\n",
    "\n",
    "ks = [1, 5, 10, 20, 30, 40, 50, 60]\n",
    "distance_strategies = [cosine_distance, jaccard_distance]\n",
    "\n",
    "thread_args = [d for d in itertools.product(distance_strategies, folds)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_global_precision(predictions, relevance_threshold=1):\n",
    "    \"\"\"\n",
    "    Returns the global precision, or micro-averaged precision, from a predictions list.\n",
    "\n",
    "    Precision is defined as the fraction of relevant instances among the retrieved instances. In recommender systems,\n",
    "    it measures the fraction of items that are liked by the user among the items that are recommended by the system.\n",
    "\n",
    "    Precision = True Positives / (True Positives + False Positives)\n",
    "\n",
    "    For example, if you have a recommender system that suggests movies to a user, and you have 100 movies in total,\n",
    "    10 of which are liked by the user. If your system recommends 8 movies to the user, 4 of which are liked by the user\n",
    "    (true positives), but also 4 of which are disliked by the user (false positives), then your precision is 4 / (4 + 4)\n",
    "    = 0.5. This means that 50% of the movies that your system recommended were actually liked by the user.\n",
    "\n",
    "    Global precision gives equal weight to each item, regardless of which user rated or was recommended it.\n",
    "    \"\"\"\n",
    "\n",
    "    def is_relevant(measure):\n",
    "        return measure >= relevance_threshold\n",
    "\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "\n",
    "    for _, _, true_rating, estimate, _ in predictions:\n",
    "        if is_relevant(estimate):\n",
    "            if is_relevant(true_rating):\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "\n",
    "    return true_positives / (true_positives + false_positives)\n",
    "\n",
    "\n",
    "def get_user_averaged_precision(predictions, relevance_threshold=1):\n",
    "    def is_relevant(measure):\n",
    "        return measure >= relevance_threshold\n",
    "\n",
    "    precisions = []\n",
    "    ratings_per_user = defaultdict(list)\n",
    "    for user_id, _, true_rating, estimate, _ in predictions:\n",
    "        ratings_per_user[user_id].append((estimate, true_rating))\n",
    "\n",
    "    for _, user_ratings in ratings_per_user.items():\n",
    "        true_positives = 0\n",
    "        false_positives = 0\n",
    "\n",
    "        for estimate, true_rating in user_ratings:\n",
    "            if is_relevant(estimate):\n",
    "                if is_relevant(true_rating):\n",
    "                    true_positives += 1\n",
    "                else:\n",
    "                    false_positives += 1\n",
    "\n",
    "        try:\n",
    "            precision = true_positives / (true_positives + false_positives)\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        else:\n",
    "            precisions.append(precision)\n",
    "\n",
    "    return statistics.mean(precisions)\n",
    "\n",
    "\n",
    "def get_precision_at_k(predictions, relevance_threshold=1, k=20):\n",
    "    def is_relevant(measure):\n",
    "        return measure >= relevance_threshold\n",
    "\n",
    "    precisions = []\n",
    "    ratings_per_user = defaultdict(list)\n",
    "    for user_id, _, true_rating, estimate, _ in predictions:\n",
    "        ratings_per_user[user_id].append((estimate, true_rating))\n",
    "\n",
    "    for _, user_ratings in ratings_per_user.items():\n",
    "        relevant_itens_in_the_top_k = 0\n",
    "\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        for estimate, true_rating in user_ratings[:k]:\n",
    "            if is_relevant(true_rating):\n",
    "                relevant_itens_in_the_top_k += 1\n",
    "\n",
    "        precisions.append(relevant_itens_in_the_top_k / k)\n",
    "\n",
    "    return statistics.mean(precisions)\n",
    "\n",
    "\n",
    "def get_global_recall(predictions, relevance_threshold=1):\n",
    "    \"\"\"\n",
    "    Returns the recall from a predictions list.\n",
    "\n",
    "    Recall is defined as the fraction of relevant instances that were retrieved. In recommender systems,\n",
    "    it measures the fraction of items that are liked by the user among all the items that are available.\n",
    "\n",
    "    Recall = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "    For example, if you have a recommender system that suggests movies to a user, and you have 100 movies in total,\n",
    "    10 of which are liked by the user. If your system misses 6 movies that are liked by the user (false negatives), then\n",
    "    your recall is 4 / (4 + 6) = 0.4. This means that 40% of the movies that are actually liked by the user were\n",
    "    recommended by your system.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def is_relevant(measure):\n",
    "        return measure >= relevance_threshold\n",
    "\n",
    "    true_positives = 0\n",
    "    false_negatives = 0\n",
    "\n",
    "    for _, _, true_rating, estimate, _ in predictions:\n",
    "        if is_relevant(estimate):\n",
    "            if is_relevant(true_rating):\n",
    "                true_positives += 1\n",
    "        else:\n",
    "            if is_relevant(true_rating):\n",
    "                false_negatives += 1\n",
    "\n",
    "    return true_positives / (true_positives + false_negatives)\n",
    "\n",
    "\n",
    "def get_user_averaged_recall(predictions, relevance_threshold=1):\n",
    "    def is_relevant(measure):\n",
    "        return measure >= relevance_threshold\n",
    "\n",
    "    recalls = []\n",
    "    ratings_per_user = defaultdict(list)\n",
    "    for user_id, _, true_rating, estimate, _ in predictions:\n",
    "        ratings_per_user[user_id].append((estimate, true_rating))\n",
    "\n",
    "    for _, user_ratings in ratings_per_user.items():\n",
    "        true_positives = 0\n",
    "        false_negatives = 0\n",
    "\n",
    "        for estimate, true_rating in user_ratings:\n",
    "            if is_relevant(estimate):\n",
    "                if is_relevant(true_rating):\n",
    "                    true_positives += 1\n",
    "            else:\n",
    "                if is_relevant(true_rating):\n",
    "                    false_negatives += 1\n",
    "        try:\n",
    "            recall = true_positives / (true_positives + false_negatives)\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "        else:\n",
    "            recalls.append(recall)\n",
    "\n",
    "    return statistics.mean(recalls)\n",
    "\n",
    "\n",
    "def get_recall_at_k(predictions, relevance_threshold=1, k=20):\n",
    "    def is_relevant(measure):\n",
    "        return measure >= relevance_threshold\n",
    "\n",
    "    recalls = []\n",
    "    ratings_per_user = defaultdict(list)\n",
    "    for user_id, _, true_rating, estimate, _ in predictions:\n",
    "        ratings_per_user[user_id].append((estimate, true_rating))\n",
    "\n",
    "    for _, user_ratings in ratings_per_user.items():\n",
    "        relevant_itens_in_the_top_k = 0\n",
    "        total_relevant_itens = 0\n",
    "\n",
    "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        for estimate, true_rating in user_ratings[:k]:\n",
    "            if is_relevant(true_rating):\n",
    "                relevant_itens_in_the_top_k += 1\n",
    "\n",
    "        for estimate, true_rating in user_ratings:\n",
    "            if is_relevant(true_rating):\n",
    "                total_relevant_itens += 1\n",
    "        try:\n",
    "            recalls.append(relevant_itens_in_the_top_k / total_relevant_itens)\n",
    "        except ZeroDivisionError:\n",
    "            pass\n",
    "\n",
    "    return statistics.mean(recalls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.BooleanMatrixBasedRecomenders import FcaBmf\n",
    "\n",
    "\n",
    "def work(distance_strategy, dataset_tuple):\n",
    "    result = {}\n",
    "\n",
    "    (index, trainset, binary_dataset, testset) = dataset_tuple\n",
    "\n",
    "    algo = FcaBmf(distance_strategy=distance_strategy)\n",
    "    algo.fit(trainset)\n",
    "\n",
    "\n",
    "    result[\"index\"] = index\n",
    "    result[\"distance_strategy\"] = distance_strategy\n",
    "    result[\"algo\"] = algo\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "with Pool(16) as pool:\n",
    "    raw_results = pool.starmap(work, iterable=thread_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidated = {d: {} for d in range(5)}\n",
    "\n",
    "for result in raw_results:\n",
    "    consolidated[result[\"index\"]][f\"fcabmf_{result['distance_strategy'].__name__}\"] = result[\"algo\"]\n",
    "\n",
    "print(consolidated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms import KNNBasic\n",
    "\n",
    "for index, trainset, binary_dataset, testset in folds:\n",
    "    KNN_recommender = KNNBasic(k=5, sim_options={\"name\": \"cosine\"})\n",
    "    KNN_recommender.fit(trainset)\n",
    "\n",
    "    consolidated[index][\"knn\"] = KNN_recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.BooleanMatrixBasedRecomenders import BinapsRecommender\n",
    "\n",
    "# for index, trainset, testset in folds:\n",
    "#     binaps_recommender = BinapsRecommender(\n",
    "#         epochs=50000, binarization_threshold=0.4, distance_strategy=jaccard_distance\n",
    "#     )\n",
    "#     binaps_recommender.fit(trainset)\n",
    "#     consolidated[index][\"binaps_recommender_jaccard\"] = binaps_recommender\n",
    "\n",
    "#     binaps_recommender = BinapsRecommender(\n",
    "#         epochs=50000, binarization_threshold=0.4, distance_strategy=cosine_distance\n",
    "#     )\n",
    "#     binaps_recommender.fit(trainset)\n",
    "#     consolidated[index][\"binaps_recommender_cosine\"] = binaps_recommender\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from lib.BooleanMatrixBasedRecomenders import BinapsRecommender, cosine_distance, jaccard_distance\n",
    "from fca.FormalConceptAnalysis import construct_context_from_binaps_patterns\n",
    "from lib.BinapsWrapper import get_patterns_from_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "fold_ids = [86 + i for i in range(5)]\n",
    "\n",
    "for id, (index, trainset, fold_binary_dataset, testset) in zip(fold_ids, folds):\n",
    "\n",
    "    cursor = DATABASE.cursor()\n",
    "    cursor.execute(\"SELECT * FROM binaps_experiments WHERE id = ?\", (id,))\n",
    "    (\n",
    "        id,\n",
    "        dataset,\n",
    "        train_set_size,\n",
    "        batch_size,\n",
    "        test_batch_size,\n",
    "        epochs,\n",
    "        learning_rate,\n",
    "        weight_decay,\n",
    "        gamma,\n",
    "        seed,\n",
    "        hidden_dimension,\n",
    "        serialized_weights,\n",
    "        serialized_training_losses,\n",
    "        serialized_test_losses,\n",
    "        runtime,\n",
    "    ) = cursor.fetchone()\n",
    "\n",
    "    weights = pickle.loads(serialized_weights)\n",
    "    training_losses = pickle.loads(serialized_training_losses)\n",
    "    test_losses = pickle.loads(serialized_test_losses)\n",
    "\n",
    "    patterns = get_patterns_from_weights(weights, 0.7)\n",
    "\n",
    "    recommender = BinapsRecommender.from_previously_computed_patterns(patterns, k=60, threshold=1, distance_strategy=jaccard_distance)\n",
    "    recommender.fit(trainset)\n",
    "\n",
    "    consolidated[index][\"binaps_recommender_jaccard\"] = recommender\n",
    "\n",
    "    recommender = BinapsRecommender.from_previously_computed_patterns(patterns, k=60, threshold=1, distance_strategy=cosine_distance)\n",
    "    recommender.fit(trainset)\n",
    "    \n",
    "    consolidated[index][\"binaps_recommender_cosine\"] = recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.accuracy import mae, rmse\n",
    "\n",
    "\n",
    "ks = [1, 5, 10, 20, 30, 40, 50, 60]\n",
    "# ks = [1, 5]\n",
    "\n",
    "RELEVANCE_THRESHOLD = 3\n",
    "NUMBER_OF_TOP_RECOMMENDATIONS = 20\n",
    "\n",
    "recommenders = consolidated[0].keys()\n",
    "\n",
    "results = {}\n",
    "for recommender in recommenders:\n",
    "    results[recommender] = {}\n",
    "    for k in ks:\n",
    "        results[recommender][k] = defaultdict(list)\n",
    "\n",
    "\n",
    "for recommender in recommenders:\n",
    "    for k in ks:\n",
    "        for index, trainset, _, testset in folds:\n",
    "            recommender_object = consolidated[index][recommender]\n",
    "            recommender_object.k = k\n",
    "            predictions = recommender_object.test(testset)\n",
    "\n",
    "            predictions = prune_impossible_predictions(predictions)\n",
    "\n",
    "            results[recommender][k][\"maes\"].append(mae(predictions=predictions, verbose=False))\n",
    "            results[recommender][k][\"rmses\"].append(rmse(predictions=predictions, verbose=False))\n",
    "\n",
    "            results[recommender][k][\"global_recalls\"].append(\n",
    "                get_global_recall(predictions=predictions, relevance_threshold=RELEVANCE_THRESHOLD)\n",
    "            )\n",
    "            results[recommender][k][\"user_averaged_recalls\"].append(\n",
    "                get_user_averaged_recall(\n",
    "                    predictions=predictions, relevance_threshold=RELEVANCE_THRESHOLD\n",
    "                )\n",
    "            )\n",
    "            results[recommender][k][\"recalls_at_k\"].append(\n",
    "                get_recall_at_k(\n",
    "                    predictions=predictions,\n",
    "                    relevance_threshold=RELEVANCE_THRESHOLD,\n",
    "                    k=NUMBER_OF_TOP_RECOMMENDATIONS,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            results[recommender][k][\"global_precisions\"].append(\n",
    "                get_global_precision(\n",
    "                    predictions=predictions, relevance_threshold=RELEVANCE_THRESHOLD\n",
    "                )\n",
    "            )\n",
    "            results[recommender][k][\"user_averaged_precisions\"].append(\n",
    "                get_user_averaged_precision(\n",
    "                    predictions=predictions, relevance_threshold=RELEVANCE_THRESHOLD\n",
    "                )\n",
    "            )\n",
    "            results[recommender][k][\"precisions_at_k\"].append(\n",
    "                get_precision_at_k(\n",
    "                    predictions=predictions,\n",
    "                    relevance_threshold=RELEVANCE_THRESHOLD,\n",
    "                    k=NUMBER_OF_TOP_RECOMMENDATIONS,\n",
    "                )\n",
    "            )\n",
    "\n",
    "\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "mae_curves = defaultdict(list)\n",
    "rmse_curves = defaultdict(list)\n",
    "\n",
    "global_recall_curves = defaultdict(list)\n",
    "user_averaged_recall_curves = defaultdict(list)\n",
    "recalls_at_k_curve = defaultdict(list)\n",
    "\n",
    "global_precision_curves = defaultdict(list)\n",
    "user_averaged_precision_curves = defaultdict(list)\n",
    "precisions_at_k_curve = defaultdict(list)\n",
    "\n",
    "for recommender in recommenders:\n",
    "    for k in ks:\n",
    "        rmse_curves[recommender].append(statistics.mean(results[recommender][k][\"rmses\"]))\n",
    "        mae_curves[recommender].append(statistics.mean(results[recommender][k][\"maes\"]))\n",
    "\n",
    "        global_recall_curves[recommender].append(\n",
    "            statistics.mean(results[recommender][k][\"global_recalls\"])\n",
    "        )\n",
    "        user_averaged_recall_curves[recommender].append(\n",
    "            statistics.mean(results[recommender][k][\"user_averaged_recalls\"])\n",
    "        )\n",
    "        recalls_at_k_curve[recommender].append(\n",
    "            statistics.mean(results[recommender][k][\"recalls_at_k\"])\n",
    "        )\n",
    "\n",
    "        global_precision_curves[recommender].append(\n",
    "            statistics.mean(results[recommender][k][\"global_precisions\"])\n",
    "        )\n",
    "        user_averaged_precision_curves[recommender].append(\n",
    "            statistics.mean(results[recommender][k][\"user_averaged_precisions\"])\n",
    "        )\n",
    "        precisions_at_k_curve[recommender].append(\n",
    "            statistics.mean(results[recommender][k][\"precisions_at_k\"])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# for recommender in recommenders:\n",
    "\n",
    "\n",
    "#     fig.add_trace(go.Scatter(x=ks, y=mae_curves[recommender], mode='lines+markers', name=recommender, showlegend=False), row=1, col=1)\n",
    "#     fig.add_trace(go.Scatter(x=ks, y=rmse_curves[recommender], mode='lines+markers', name=recommender, showlegend=False), row=1, col=2)\n",
    "\n",
    "#     fig.add_trace(go.Scatter(x=ks, y=global_recall_curves[recommender], mode='lines+markers', name=recommender, showlegend=False), row=2, col=1)\n",
    "#     fig.add_trace(go.Scatter(x=ks, y=user_averaged_recall_curves[recommender], mode='lines+markers', name=recommender, showlegend=False), row=2, col=2)\n",
    "\n",
    "#     fig.add_trace(go.Scatter(x=ks, y=global_precision_curves[recommender], mode='lines+markers', name=recommender, showlegend=False), row=3, col=1)\n",
    "#     fig.add_trace(go.Scatter(x=ks, y=user_averaged_precision_curves[recommender], mode='lines+markers', name=recommender, showlegend=False), row=3, col=2)\n",
    "\n",
    "#     fig.add_trace(go.Scatter(x=ks, y=recalls_at_k_curve[recommender], mode='lines+markers', name=recommender, showlegend=False), row=4, col=1)\n",
    "#     fig.add_trace(go.Scatter(x=ks, y=precisions_at_k_curve[recommender], mode='lines+markers', name=recommender, showlegend=False), row=4, col=2)\n",
    "\n",
    "\n",
    "# fig.show()\n",
    "label = {\n",
    "    \"fcabmf_cosine_distance\": \"GreConD / Cosine Similarity\",\n",
    "    \"fcabmf_jaccard_distance\": \"GreConD / Jaccard Distance\",\n",
    "    \"knn\": \"KNN / Cosine Similarity\",\n",
    "    \"binaps_recommender_jaccard\": \"BinaPs / Jaccard Distance\",\n",
    "    \"binaps_recommender_cosine\": \"BinaPs / Cosine Similarity\",\n",
    "}\n",
    "\n",
    "colors = {\n",
    "    \"fcabmf_cosine_distance\": dict(color=\"red\", dash=\"dot\", width=4),\n",
    "    \"fcabmf_jaccard_distance\": dict(color=\"red\", dash=\"solid\", width=4),\n",
    "    \"knn\": dict(color=\"green\", dash=\"dot\", width=4),\n",
    "    \"binaps_recommender_jaccard\": dict(color=\"blue\", dash=\"solid\", width=4),\n",
    "    \"binaps_recommender_cosine\": dict(color=\"blue\", dash=\"dot\", width=4),\n",
    "}\n",
    "\n",
    "\n",
    "def generate_plot(data, title):\n",
    "    fig = go.Figure()\n",
    "\n",
    "    for recommender in recommenders:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=ks, y=data[recommender], name=label[recommender], line=colors[recommender])\n",
    "        )\n",
    "\n",
    "    fig.update_traces(showlegend=True)\n",
    "    fig.update_layout(height=HEIGHT / 2, margin_l=80)\n",
    "    fig.update_xaxes(title=\"K\")\n",
    "    fig.update_yaxes(title=title)\n",
    "    fig.show(renderer=\"png\")\n",
    "    fig.write_image(f\"{title}.png\", format=FORMAT, scale=DPI / 96)\n",
    "\n",
    "\n",
    "generate_plot(mae_curves, \"MAE\")\n",
    "generate_plot(rmse_curves, \"RMSE\")\n",
    "generate_plot(global_recall_curves, \"Global Recall\")\n",
    "generate_plot(user_averaged_recall_curves, \"User Averaged Recall\")\n",
    "generate_plot(global_precision_curves, \"Global Precision\")\n",
    "generate_plot(user_averaged_precision_curves, \"User Averaged Precision\")\n",
    "generate_plot(recalls_at_k_curve, \"Recall@N\")\n",
    "generate_plot(precisions_at_k_curve, \"Precision@N\")\n",
    "\n",
    "# fig.write_image(\"top_k_correlation.png\", format=FORMAT, scale=DPI/96)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binaps",
   "language": "python",
   "name": "binaps"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
