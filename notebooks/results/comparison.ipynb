{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the autoreload extension\n",
    "%load_ext autoreload\n",
    "\n",
    "# Set autoreload to reload all modules every time a cell is executed\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assert reproducibility of the results\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 0\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from scripts.generate_movielens_folds import download_movielens, generate_folds\n",
    "\n",
    "output_dir = Path(\"/tmp/folds\")\n",
    "movielens_path = output_dir / \"ml-100k\"\n",
    "\n",
    "download_movielens(output_dir)\n",
    "generate_folds(movielens_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import Dataset, Reader\n",
    "from surprise.model_selection import PredefinedKFold\n",
    "\n",
    "reader = Reader(\"ml-100k\")\n",
    "\n",
    "folds_files = [\n",
    "    (movielens_path / f\"u{i}.base\", movielens_path / f\"u{i}.test\") for i in (1, 2, 3, 4, 5)\n",
    "]\n",
    "\n",
    "data = Dataset.load_from_folds(folds_files, reader=reader)\n",
    "pkf = PredefinedKFold()\n",
    "\n",
    "# Add index to the folds so it easier to track\n",
    "folds = [(index, fold) for index, fold in enumerate(pkf.split(data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10000                           # Number of epochs to be used in the training of the models\n",
    "WEIGHTS_BINARIZATION_THRESHOLD = 0.7    # Threshold that contros which weights are going to be considered part of an itemset\n",
    "BINARIZATION_THRESHOLD = 4              # Threshold for a rating to be considered True\n",
    "KNN_K = 20                              # Number of neighbors to be considered in the KNN algorithm or KNN-based algorithms\n",
    "RELEVANCE_THRESHOLD = 4                 # Threshold for a rating to be considered relevant or selected\n",
    "NUMBER_OF_TOP_RECOMMENDATIONS = 20      # Number of top recommendations to be considered in Precision@K and Recall@K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import statistics\n",
    "from multiprocessing import Pool, Manager, cpu_count\n",
    "\n",
    "from recommenders.grecond_recommender import GreConDRecommender\n",
    "from recommenders.common import cosine_distance\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "manager = Manager()\n",
    "output = manager.dict()\n",
    "\n",
    "# Construct thread arguments\n",
    "recommenders = [\n",
    "    GreConDRecommender(\n",
    "        dataset_binarization_threshold=BINARIZATION_THRESHOLD,\n",
    "        knn_k=KNN_K,\n",
    "        knn_distance_strategy=cosine_distance,\n",
    "    )\n",
    "]\n",
    "thread_args = [\n",
    "    args\n",
    "    for args in itertools.product(\n",
    "        folds, [output], recommenders, [RELEVANCE_THRESHOLD], [NUMBER_OF_TOP_RECOMMENDATIONS]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Run the threads\n",
    "start_time = time.time()\n",
    "with Pool(cpu_count()) as pool:\n",
    "    pool.starmap(GreConDRecommender.thread, iterable=thread_args)\n",
    "end_time = time.time()\n",
    "grecond_runtime = end_time - start_time\n",
    "\n",
    "# Compile and calculate the mean of the metrics\n",
    "grecond_actual_coverage = []\n",
    "grecond_number_of_factors = []\n",
    "grecond_mae = []\n",
    "grecond_rmse = []\n",
    "grecond_micro_averaged_recall = []\n",
    "grecond_macro_averaged_recall = []\n",
    "grecond_recall_at_k = []\n",
    "grecond_micro_averaged_precision = []\n",
    "grecond_macro_averaged_precision = []\n",
    "grecond_precision_at_k = []\n",
    "\n",
    "for key, value in output.items():\n",
    "    grecond_coverage, _, _ = key\n",
    "    grecond_actual_coverage.append(value[\"actual_coverage\"])\n",
    "    grecond_number_of_factors.append(value[\"number_of_factors\"])\n",
    "    grecond_mae.append(value[\"mae\"])\n",
    "    grecond_rmse.append(value[\"rmse\"])\n",
    "    grecond_micro_averaged_recall.append(value[\"micro_averaged_recall\"])\n",
    "    grecond_macro_averaged_recall.append(value[\"macro_averaged_recall\"])\n",
    "    grecond_recall_at_k.append(value[\"recall_at_k\"])\n",
    "    grecond_micro_averaged_precision.append(value[\"micro_averaged_precision\"])\n",
    "    grecond_macro_averaged_precision.append(value[\"macro_averaged_precision\"])\n",
    "    grecond_precision_at_k.append(value[\"precision_at_k\"])\n",
    "\n",
    "mean_grecond_actual_coverage = statistics.mean(grecond_actual_coverage)\n",
    "mean_grecond_number_of_factors = statistics.mean(grecond_number_of_factors)\n",
    "mean_grecond_mae = statistics.mean(grecond_mae)\n",
    "mean_grecond_rmse = statistics.mean(grecond_rmse)\n",
    "mean_grecond_micro_averaged_recall = statistics.mean(grecond_micro_averaged_recall)\n",
    "mean_grecond_macro_averaged_recall = statistics.mean(grecond_macro_averaged_recall)\n",
    "mean_grecond_recall_at_k = statistics.mean(grecond_recall_at_k)\n",
    "mean_grecond_micro_averaged_precision = statistics.mean(grecond_micro_averaged_precision)\n",
    "mean_grecond_macro_averaged_precision = statistics.mean(grecond_macro_averaged_precision)\n",
    "mean_grecond_precision_at_k = statistics.mean(grecond_precision_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms import KNNBasic\n",
    "from recommenders.common import generic_thread\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "output = manager.dict()\n",
    "\n",
    "# Construct thread arguments\n",
    "recommenders = [KNNBasic(k=KNN_K, sim_options={\"name\": \"cosine\"})]\n",
    "thread_args = [\n",
    "    args\n",
    "    for args in itertools.product(\n",
    "        folds, [output], recommenders, [RELEVANCE_THRESHOLD], [NUMBER_OF_TOP_RECOMMENDATIONS]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Run the threads\n",
    "start_time = time.time()\n",
    "with Pool(cpu_count()) as pool:\n",
    "    pool.starmap(generic_thread, iterable=thread_args)\n",
    "end_time = time.time()\n",
    "knn_runtime = end_time - start_time\n",
    "\n",
    "# Compile and calculate the mean of the metrics\n",
    "knn_mae = []\n",
    "knn_rmse = []\n",
    "knn_micro_averaged_recall = []\n",
    "knn_macro_averaged_recall = []\n",
    "knn_recall_at_k = []\n",
    "knn_micro_averaged_precision = []\n",
    "knn_macro_averaged_precision = []\n",
    "knn_precision_at_k = []\n",
    "\n",
    "for key, value in output.items():\n",
    "    knn_mae.append(value[\"mae\"])\n",
    "    knn_rmse.append(value[\"rmse\"])\n",
    "    knn_micro_averaged_recall.append(value[\"micro_averaged_recall\"])\n",
    "    knn_macro_averaged_recall.append(value[\"macro_averaged_recall\"])\n",
    "    knn_recall_at_k.append(value[\"recall_at_k\"])\n",
    "    knn_micro_averaged_precision.append(value[\"micro_averaged_precision\"])\n",
    "    knn_macro_averaged_precision.append(value[\"macro_averaged_precision\"])\n",
    "    knn_precision_at_k.append(value[\"precision_at_k\"])\n",
    "\n",
    "mean_knn_mae = statistics.mean(knn_mae)\n",
    "mean_knn_rmse = statistics.mean(knn_rmse)\n",
    "mean_knn_micro_averaged_recall = statistics.mean(knn_micro_averaged_recall)\n",
    "mean_knn_macro_averaged_recall = statistics.mean(knn_macro_averaged_recall)\n",
    "mean_knn_recall_at_k = statistics.mean(knn_recall_at_k)\n",
    "mean_knn_micro_averaged_precision = statistics.mean(knn_micro_averaged_precision)\n",
    "mean_knn_macro_averaged_precision = statistics.mean(knn_macro_averaged_precision)\n",
    "mean_knn_precision_at_k = statistics.mean(knn_precision_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms import SVD\n",
    "from recommenders.common import generic_thread\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "output = manager.dict()\n",
    "\n",
    "# Construct thread arguments\n",
    "recommenders = [SVD()]\n",
    "thread_args = [\n",
    "    args\n",
    "    for args in itertools.product(\n",
    "        folds, [output], recommenders, [RELEVANCE_THRESHOLD], [NUMBER_OF_TOP_RECOMMENDATIONS]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Run the threads\n",
    "start_time = time.time()\n",
    "with Pool(cpu_count()) as pool:\n",
    "    pool.starmap(generic_thread, iterable=thread_args)\n",
    "end_time = time.time()\n",
    "svd_runtime = end_time - start_time\n",
    "\n",
    "# Compile and calculate the mean of the metrics\n",
    "svd_mae = []\n",
    "svd_rmse = []\n",
    "svd_micro_averaged_recall = []\n",
    "svd_macro_averaged_recall = []\n",
    "svd_recall_at_k = []\n",
    "svd_micro_averaged_precision = []\n",
    "svd_macro_averaged_precision = []\n",
    "svd_precision_at_k = []\n",
    "\n",
    "for key, value in output.items():\n",
    "    svd_mae.append(value[\"mae\"])\n",
    "    svd_rmse.append(value[\"rmse\"])\n",
    "    svd_micro_averaged_recall.append(value[\"micro_averaged_recall\"])\n",
    "    svd_macro_averaged_recall.append(value[\"macro_averaged_recall\"])\n",
    "    svd_recall_at_k.append(value[\"recall_at_k\"])\n",
    "    svd_micro_averaged_precision.append(value[\"micro_averaged_precision\"])\n",
    "    svd_macro_averaged_precision.append(value[\"macro_averaged_precision\"])\n",
    "    svd_precision_at_k.append(value[\"precision_at_k\"])\n",
    "\n",
    "mean_svd_mae = statistics.mean(svd_mae)\n",
    "mean_svd_rmse = statistics.mean(svd_rmse)\n",
    "mean_svd_micro_averaged_recall = statistics.mean(svd_micro_averaged_recall)\n",
    "mean_svd_macro_averaged_recall = statistics.mean(svd_macro_averaged_recall)\n",
    "mean_svd_recall_at_k = statistics.mean(svd_recall_at_k)\n",
    "mean_svd_micro_averaged_precision = statistics.mean(svd_micro_averaged_precision)\n",
    "mean_svd_macro_averaged_precision = statistics.mean(svd_macro_averaged_precision)\n",
    "mean_svd_precision_at_k = statistics.mean(svd_precision_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.pedro import PedroRecommender\n",
    "from recommenders.common import generic_thread\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "output = manager.dict()\n",
    "\n",
    "# Construct thread arguments\n",
    "recommenders = [\n",
    "    PedroRecommender(\n",
    "        epochs=EPOCHS,\n",
    "        weights_binarization_threshold=WEIGHTS_BINARIZATION_THRESHOLD,\n",
    "        dataset_binarization_threshold=BINARIZATION_THRESHOLD,\n",
    "    )\n",
    "]\n",
    "thread_args = [\n",
    "    args\n",
    "    for args in itertools.product(\n",
    "        folds, [output], recommenders, [RELEVANCE_THRESHOLD], [NUMBER_OF_TOP_RECOMMENDATIONS]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Run the threads\n",
    "start_time = time.time()\n",
    "with Pool(cpu_count()) as pool:\n",
    "    pool.starmap(generic_thread, iterable=thread_args)\n",
    "end_time = time.time()\n",
    "pedro_runtime = end_time - start_time\n",
    "\n",
    "# Compile and calculate the mean of the metrics\n",
    "pedro_mae = []\n",
    "pedro_rmse = []\n",
    "pedro_micro_averaged_recall = []\n",
    "pedro_macro_averaged_recall = []\n",
    "pedro_recall_at_k = []\n",
    "pedro_micro_averaged_precision = []\n",
    "pedro_macro_averaged_precision = []\n",
    "pedro_precision_at_k = []\n",
    "\n",
    "for key, value in output.items():\n",
    "    pedro_mae.append(value[\"mae\"])\n",
    "    pedro_rmse.append(value[\"rmse\"])\n",
    "    pedro_micro_averaged_recall.append(value[\"micro_averaged_recall\"])\n",
    "    pedro_macro_averaged_recall.append(value[\"macro_averaged_recall\"])\n",
    "    pedro_recall_at_k.append(value[\"recall_at_k\"])\n",
    "    pedro_micro_averaged_precision.append(value[\"micro_averaged_precision\"])\n",
    "    pedro_macro_averaged_precision.append(value[\"macro_averaged_precision\"])\n",
    "    pedro_precision_at_k.append(value[\"precision_at_k\"])\n",
    "\n",
    "mean_pedro_mae = statistics.mean(pedro_mae)\n",
    "mean_pedro_rmse = statistics.mean(pedro_rmse)\n",
    "mean_pedro_micro_averaged_recall = statistics.mean(pedro_micro_averaged_recall)\n",
    "mean_pedro_macro_averaged_recall = statistics.mean(pedro_macro_averaged_recall)\n",
    "mean_pedro_recall_at_k = statistics.mean(pedro_recall_at_k)\n",
    "mean_pedro_micro_averaged_precision = statistics.mean(pedro_micro_averaged_precision)\n",
    "mean_pedro_macro_averaged_precision = statistics.mean(pedro_macro_averaged_precision)\n",
    "mean_pedro_precision_at_k = statistics.mean(pedro_precision_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recommenders.binaps_recommender import BinaPsRecommender\n",
    "from recommenders.common import generic_thread, cosine_distance\n",
    "\n",
    "# Create a dictionary to store the results\n",
    "output = manager.dict()\n",
    "\n",
    "# Construct thread arguments\n",
    "recommenders = [\n",
    "    BinaPsRecommender(\n",
    "        epochs=EPOCHS,\n",
    "        dataset_binarization_threshold=BINARIZATION_THRESHOLD,\n",
    "        weights_binarization_threshold=WEIGHTS_BINARIZATION_THRESHOLD,\n",
    "        knn_k=KNN_K,\n",
    "        knn_distance_strategy=cosine_distance,\n",
    "    )\n",
    "]\n",
    "thread_args = [\n",
    "    args\n",
    "    for args in itertools.product(\n",
    "        folds, [output], recommenders, [RELEVANCE_THRESHOLD], [NUMBER_OF_TOP_RECOMMENDATIONS]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Run the threads\n",
    "start_time = time.time()\n",
    "with Pool(cpu_count()) as pool:\n",
    "    pool.starmap(generic_thread, iterable=thread_args)\n",
    "end_time = time.time()\n",
    "binaps_runtime = end_time - start_time\n",
    "\n",
    "# Compile and calculate the mean of the metrics\n",
    "binaps_mae = []\n",
    "binaps_rmse = []\n",
    "binaps_micro_averaged_recall = []\n",
    "binaps_macro_averaged_recall = []\n",
    "binaps_recall_at_k = []\n",
    "binaps_micro_averaged_precision = []\n",
    "binaps_macro_averaged_precision = []\n",
    "binaps_precision_at_k = []\n",
    "\n",
    "for key, value in output.items():\n",
    "    binaps_mae.append(value[\"mae\"])\n",
    "    binaps_rmse.append(value[\"rmse\"])\n",
    "    binaps_micro_averaged_recall.append(value[\"micro_averaged_recall\"])\n",
    "    binaps_macro_averaged_recall.append(value[\"macro_averaged_recall\"])\n",
    "    binaps_recall_at_k.append(value[\"recall_at_k\"])\n",
    "    binaps_micro_averaged_precision.append(value[\"micro_averaged_precision\"])\n",
    "    binaps_macro_averaged_precision.append(value[\"macro_averaged_precision\"])\n",
    "    binaps_precision_at_k.append(value[\"precision_at_k\"])\n",
    "\n",
    "mean_binaps_mae = statistics.mean(binaps_mae)\n",
    "mean_binaps_rmse = statistics.mean(binaps_rmse)\n",
    "mean_binaps_micro_averaged_recall = statistics.mean(binaps_micro_averaged_recall)\n",
    "mean_binaps_macro_averaged_recall = statistics.mean(binaps_macro_averaged_recall)\n",
    "mean_binaps_recall_at_k = statistics.mean(binaps_recall_at_k)\n",
    "mean_binaps_micro_averaged_precision = statistics.mean(binaps_micro_averaged_precision)\n",
    "mean_binaps_macro_averaged_precision = statistics.mean(binaps_macro_averaged_precision)\n",
    "mean_binaps_precision_at_k = statistics.mean(binaps_precision_at_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"GreConD-KNN\": [\n",
    "            mean_grecond_mae,\n",
    "            mean_grecond_rmse,\n",
    "            mean_grecond_micro_averaged_recall,\n",
    "            mean_grecond_macro_averaged_recall,\n",
    "            mean_grecond_recall_at_k,\n",
    "            mean_grecond_micro_averaged_precision,\n",
    "            mean_grecond_macro_averaged_precision,\n",
    "            mean_grecond_precision_at_k,\n",
    "            grecond_runtime\n",
    "        ],\n",
    "        \"KNN (Surprise)\": [\n",
    "            mean_knn_mae,\n",
    "            mean_knn_rmse,\n",
    "            mean_knn_micro_averaged_recall,\n",
    "            mean_knn_macro_averaged_recall,\n",
    "            mean_knn_recall_at_k,\n",
    "            mean_knn_micro_averaged_precision,\n",
    "            mean_knn_macro_averaged_precision,\n",
    "            mean_knn_precision_at_k,\n",
    "            knn_runtime\n",
    "        ],\n",
    "        \"SVD (Surprise)\": [\n",
    "            mean_svd_mae,\n",
    "            mean_svd_rmse,\n",
    "            mean_svd_micro_averaged_recall,\n",
    "            mean_svd_macro_averaged_recall,\n",
    "            mean_svd_recall_at_k,\n",
    "            mean_svd_micro_averaged_precision,\n",
    "            mean_svd_macro_averaged_precision,\n",
    "            mean_svd_precision_at_k,\n",
    "            svd_runtime\n",
    "        ],\n",
    "        \"Pedro\": [\n",
    "            mean_pedro_mae,\n",
    "            mean_pedro_rmse,\n",
    "            mean_pedro_micro_averaged_recall,\n",
    "            mean_pedro_macro_averaged_recall,\n",
    "            mean_pedro_recall_at_k,\n",
    "            mean_pedro_micro_averaged_precision,\n",
    "            mean_pedro_macro_averaged_precision,\n",
    "            mean_pedro_precision_at_k,\n",
    "            pedro_runtime\n",
    "        ],\n",
    "        \"Bernardo\": [\n",
    "            mean_binaps_mae,\n",
    "            mean_binaps_rmse,\n",
    "            mean_binaps_micro_averaged_recall,\n",
    "            mean_binaps_macro_averaged_recall,\n",
    "            mean_binaps_recall_at_k,\n",
    "            mean_binaps_micro_averaged_precision,\n",
    "            mean_binaps_macro_averaged_precision,\n",
    "            mean_binaps_precision_at_k,\n",
    "            binaps_runtime\n",
    "        ]\n",
    "    },\n",
    "    index=[\n",
    "        \"MAE\",\n",
    "        \"RMSE\",\n",
    "        \"Micro Averaged Recall\",\n",
    "        \"Macro Averaged Recall\",\n",
    "        \"Recall@K\",\n",
    "        \"Micro Averaged Precision\",\n",
    "        \"Macro Averaged Precision\",\n",
    "        \"Precision@K\",\n",
    "        \"Runtime (in seconds)\"\n",
    "    ],\n",
    ")\n",
    "\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "# print(df)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
